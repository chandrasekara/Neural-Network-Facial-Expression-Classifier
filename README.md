**Dhilan Chandrasekara**

A collection of projects completed as part of the coursework for Udacity's Machine Learning Engineer Nanodegree. 

These projects were completed to reinforce knowledge in the following areas: Supervised Learning, Unsupervised Learning, Reinforcement Learning and Deep Learning

The capstone project was started and completed from scratch, by myself. For the other projects, skeleton code and helpful prompts were provided by the team at Udacity, in order to assist the development process.

**Capstone Project - Facial Expression Classifier**

-Built a convolutional neural network architecture in Keras to process images of faces, and classify the facial expression of the person into one of the following categories: fearful, happy, sad, digusted, angry, neutral, surprised.
-Trained various CNN architectures on GPU-compute servers with Amazon Web Services.
-Produced visualizations of the training process
-Compiled the process and findings of this project into an academic style report

**Supervised Learning - Prediction of Housing Prices in Boston**
	
-Utilized historical housing data from Boston to predict prices of unknown houses<br>
-Made use of linear regression to draw relationships between house and neighbourhood characteristics

**Unsupervised Learning - Identifying Customer Segments for a Wholesale Distributor**
	
-Used clustering algorithms to split the large dataset into different segments, corresponding to different customer types<br>
-Investigated use of Principal Component Analysis to aid with performance of clustering algorithms and for visualization purposes

**Reinforcement Learning - Training a Self Driving Car in a Simulation**

-Investigated use of Q-learning to train a smartcab to drive safely and optimally in a simulation

**Deep Learning - Dog Breed Classifier**

-Utilized convolutional neural networks to recognize different dog breeds in images<br>
-Investigated the performance of many different architectures in Keras, using GPU acceleration to speed up training time<br>
-Utilized transfer learning by making use of pre-trained networks, such as the VGG16 and Resnet-50 models

